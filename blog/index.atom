<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>http://preaction.me/blog/</id>
    <title>preaction</title>
    <updated>2017-10-18T00:00:00Z</updated>
    <link href="http://preaction.me/blog/index.atom" rel="self" />
    <link href="http://preaction.me/blog/" rel="alternate" />
    <generator version="0.085">Statocles</generator>
    <entry>
        <id>http://preaction.me/blog/2017/10/18/application-metrics-with-yertl/</id>
        <title>Application Metrics with Yertl</title>
        <link href="http://preaction.me/blog/2017/10/18/application-metrics-with-yertl/" rel="alternate" />
        <content type="html"><![CDATA[
            <p>A time series database is a massively useful tool for system reporting
and monitoring. By storing series of simple values attached to
timestamps, an ops team can see how fast their application is
processing data, how much traffic they&#39;re serving, and how many
resources they&#39;re consuming. From this data they can determine how well
their application is working, track down issues in the system, and plan
for future resource needs.</p>

<p>There have been a lot of new databases and tools developed to create,
store, and consume time series data, and existing databases are being
enhanced to better support time series data.</p>

<p>With the new release of <a href="http://preaction.me/yertl">ETL::Yertl</a>, we can
easily translate SQL database queries into metrics for monitoring and
reporting. I&#39;ve been using these new features to monitor the <a href="http://cpantesters.org">CPAN
Testers</a> application.</p>

                <p><a href="http://preaction.me/blog/2017/10/18/application-metrics-with-yertl/#section-2">Continue reading...</a></p>
            <p>Tags:
                <a href="http://preaction.me/blog/tag/yertl/">yertl</a>
                <a href="http://preaction.me/blog/tag/devops/">devops</a>
            </p>
        ]]></content>
        <updated>2017-10-18T00:00:00Z</updated>
        <category term="yertl" />
        <category term="devops" />
    </entry>
    <entry>
        <id>http://preaction.me/blog/2017/07/05/demo-a-live-web-app-on-bad-internet/</id>
        <title>Demo a Live Web App on Bad Internet</title>
        <link href="http://preaction.me/blog/2017/07/05/demo-a-live-web-app-on-bad-internet/" rel="alternate" />
        <content type="html"><![CDATA[
            <p><a href="https://opensource.com/article/17/7/squid-proxy">Originally posted on Opensource.com</a></p>

<p>Live demos are the bane of professional speakers everywhere. Even the
most well-prepared live demo can go wrong for unforeseeable reasons.
This is a bad thing to happen while you&#39;re up on-stage in front of 300
people. Live demos of remote web apps are so fraught with peril that
most people find other ways of presenting them. Screenshots can never
fail, and local sandboxes won&#39;t fail on overloaded conference Internet
connections. But what if we can&#39;t set up a local sandbox in time for our
talk? What if our database is huge and complex? What if our app has
animation and interactions that we can&#39;t show with screenshots?</p>

<p>What if we could record our use of a web application and then replay the
stored responses at the right time? Lucky for us, it&#39;s easy to proxy
HTTP, the protocol that web browsers and web servers use to communicate
with each other. This means we can put an intermediary between our
browser and the server to do whatever we want. Often caching does
content filtering (corporate filters, parental filters). But caching
data on a server closer to the user can speed up a website.</p>

<p>We&#39;re going to use a web proxy in a similar way: We&#39;ll cache our content
and serve that cached data to our web browser. However, we&#39;re going to
run our proxy on the same machine as our web browser. And, we&#39;re going
to set it up to cache only the things that we want. This way we can run
a live demo on an unstable connection.</p>

<h2>Install and Configure Squid HTTP Proxy</h2>

<p>First, we need to install and configure our proxy. I&#39;m on a Mac, so
I was able to install <a href="http://www.squid-cache.org">the Squid HTTP Proxy</a>
via <a href="https://brew.sh">Homebrew, a free package manager for MacOS</a>.</p>

<p>For our live demo, we want to cache the application we are trying to
demo and any other content the application needs. Anything else is
unnecessary. To do this, Squid has <a href="http://www.squid-cache.org/Doc/config/acl/">Access Control Lists
(ACLs)</a>. We
configure an ACL with a list of domains that we should cache, and deny
everything else. For maximum coverage, we should add both the host name
and the IP addresses to the ACL.  Since HTTP proxies are also used for
DNS, most of the time the proxy is looking up the DNS records.  But
sometimes a browser already knows the IP and will just tell the proxy to
get on with it.</p>

<p>So, here&#39;s our list of domains and IPs:</p>

<pre><code>acl cacheDomain dstdomain beta.cpantesters.org
acl cacheDomain dstdomain api.cpantesters.org
acl cacheDomain dstdomain www.cpantesters.org
acl cacheDomain dstdomain 212.110.173.51
acl cacheDomain dstdomain cdnjs.cloudflare.com
</code></pre>

<p>The first three domains are the applications that I&#39;m running. The
fourth is the IP address for that application server: All the domains
are on the same machine. The last is <a href="http://cdnjs.com">CDNJS, the JavaScript
CDN</a> that I get my JavaScript from. In order for my
application to work, I will need to cache all the JavaScript and CSS
that I depend on from CDNJS.</p>

<p>Once we&#39;ve listed what we want to cache, we can forbid any other domains
from being cached:</p>

<pre><code>cache deny !cacheDomain
</code></pre>

<p>Next, we should tell Squid where to put our cache and how much disk
space to use. Homebrew&#39;s Squid configuration has a <code>cache_dir</code> line, commented
out. We need to enable it and increase the disk space available to
ensure that our data stays cached. When the disk space is used up, Squid
starts deleting old cached data, which we can&#39;t have during our demo.</p>

<pre><code># Uncomment and adjust the following to add a disk cache directory.
cache_dir ufs /usr/local/var/cache/squid 1024 16 256
</code></pre>

<p>The first number at the end of the line is the cache size in MB, which
I adjusted to 1024 (1 GB).</p>

<p>Finally, we should make sure that we can use Squid&#39;s management API, and
that it&#39;s only open to the local machine. This should be the default, so
look for these <code>http_access</code> lines, and add them if they don&#39;t exist.</p>

<pre><code># Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager
</code></pre>

<p>After allowing cache manager access from <code>localhost</code>, we should disable
the cache manager password:</p>

<pre><code>cachemgr_passwd none all
</code></pre>

<p>Now we&#39;re done with the configuration file. Our <a href="https://gist.github.com/preaction/d90ad4b9ddbe1f390252319cbec980f9">full configuration file is located
here</a>.</p>

<p>Now that we&#39;ve configured our proxy, we can start it up. Homebrew says
to do <code>brew services start squid</code>, but your platform may need something
different. This gets the proxy started and waiting for requests. Next
we need to configure our browser to use the proxy.</p>

<h2>Configure your web browser</h2>

<p>Configuring your web browser for an HTTP proxy depends on what browser
you use and what OS you use. If you&#39;re using Chrome or Safari on MacOS,
you can go to System Preferences to configure a proxy. However, if
you&#39;re using Firefox, you can configure the browser to use a proxy, and
leave the rest of the system alone. Other operating systems have other
ways to configure proxies, and you should check your OS&#39;s documentation.</p>

<p>There are some good browser plugins for managing HTTP proxies, but
unfortunately not for Safari or IE. If you&#39;re using Chrome, try <a href="https://chrome.google.com/webstore/detail/proxy-switchyomega/padekgcemlokbadohgkifijomclgjgif">Proxy
SwitchyOmega</a>.
If you&#39;re using Firefox, use <a href="https://addons.mozilla.org/en-US/firefox/addon/foxyproxy-standard/">FoxyProxy
Standard</a>.
These plugins make it easier to manage HTTP proxies.</p>

<h2>Run through the demo to cache your content</h2>

<p>Once you configure your proxy, you can run through your demo to test it.
Do this on a good Internet connection. As you run through your demo,
your browser asks its proxy to fetch all the demo&#39;s data. As your proxy
does this, it caches it on disk. Since your computer is online, Squid
will follow the caching rules that the web server asks it to. This means
caching for a specific length of time, and possibly revalidating the
data to see if it changed.</p>

<p>As we run through our demo, we should make sure that our cache is being
used. The easiest way to do that is to read Squid&#39;s log. For my
configuration, it was located at <code>/usr/local/var/logs/access.log</code>.
Inside are lines that look like this:</p>

<pre><code>1498020228.970    203 ::1 TCP_MISS/200 3653 GET http://beta.cpantesters.org/chart.html? - HIER_DIRECT/212.110.173.51 text/html
1498020229.523    314 ::1 TCP_REFRESH_MODIFIED/200 8130 GET http://api.cpantesters.org/v3/release/dist/Statocles - HIER_DIRECT/212.110.173.51 application/json
1498020236.187   6945 ::1 TCP_MISS/200 148284 GET http://api.cpantesters.org/v3/summary/Statocles/0.077 - HIER_DIRECT/212.110.173.51 application/json
1498020240.783    186 ::1 TCP_MISS/200 6597 GET http://people.w3.org/~dom/archives/2006/09/offline-web-cache-with-squid/ - HIER_DIRECT/128.30.54.11 text/html
</code></pre>

<p>The important parts of this line are the URL and the status.
<code>TCP_MISS/200</code> means &quot;this request was not in our cache, and the remote
server returned a <code>200 OK</code> HTTP response&quot;. <code>TCP_REFRESH_MODIFIED/200</code>
means &quot;this request was in our cache, but we refreshed it from the
remote server which returned a <code>200 OK</code> HTTP response&quot;. This is our
cache building and refreshing itself because we&#39;re on a stable
connection. Once we have some data in our cache, we&#39;ll start seeing
things like this:</p>

<pre><code>1498063273.261      0 ::1 TCP_INM_HIT/304 299 GET http://beta.cpantesters.org/chart.html - HIER_NONE/- text/html
1498063281.831      0 ::1 TCP_MEM_HIT/200 8187 GET http://api.cpantesters.org/v3/release/dist/Statocles - HIER_NONE/- application/json
1498063298.103      0 ::1 TCP_MEM_HIT/200 8187 GET http://api.cpantesters.org/v3/release/dist/Statocles - HIER_NONE/- application/json
1498063300.473      8 ::1 TCP_MEM_HIT/200 154917 GET http://api.cpantesters.org/v3/summary/Statocles/0.083 - HIER_NONE/- application/json
</code></pre>

<p><code>TCP_INM_HIT/304</code> means &quot;The cache responded to this request with a <code>304
Not Modified</code> response&quot;. The <code>TCP_MEM_HIT/200</code> means &quot;The cache
responded to this request with a <code>200 OK</code> HTTP response&quot;. These are what
we want: The cache is serving responses, not the remote server.</p>

<h2>Run Your Demo</h2>

<p>Now that our cache is operating well on a stable connection, we can give
our demo on an unstable one. First, we want to make sure that our cache
does not try to access the remote server (Squid&#39;s &quot;offline&quot; mode). To do
this, Squid has a management client called <code>squidclient</code> which we can use
to toggle offline mode.</p>

<pre><code>$ squidclient mgr:offline_toggle
HTTP/1.1 200 OK
Server: squid/3.5.26
Mime-Version: 1.0
Date: Tue, 04 Jul 2017 21:16:36 GMT
Content-Type: text/plain;charset=utf-8
Expires: Tue, 04 Jul 2017 21:16:36 GMT
Last-Modified: Tue, 04 Jul 2017 21:16:36 GMT
X-Cache: MISS from gwen.local
Via: 1.1 gwen.local (squid/3.5.26)
Connection: close

offline_mode is now ON
</code></pre>

<p>Squid&#39;s offline mode minimizes attempts to get remote content. Since we
cached all our content running through our demo, this means Squid will
be serving our demo!</p>

<p>So now we can run our demo worry-free! All the remote content is served
by the local machine, so it doesn&#39;t matter how good the conference wi-fi
is. As long as stick to things we&#39;ve already cached, our web application
runs perfectly.</p>

                <p><a href="http://preaction.me/blog/2017/07/05/demo-a-live-web-app-on-bad-internet/#section-2">Continue reading...</a></p>
            <p>Tags:
            </p>
        ]]></content>
        <updated>2017-07-05T00:00:00Z</updated>
    </entry>
    <entry>
        <id>http://preaction.me/blog/2017/07/04/cpan-testers-has-an-api/</id>
        <title>CPAN Testers Has an API</title>
        <link href="http://preaction.me/blog/2017/07/04/cpan-testers-has-an-api/" rel="alternate" />
        <content type="html"><![CDATA[
            <p>[<a href="https://www.youtube.com/watch?v=okeBgBb1Cxs&amp;index=60&amp;list=PLA9_Hq3zhoFxdSVDA4v9Af3iutQxLI14m">Watch this lightning talk on The Perl Conference YouTube
channel</a>]</p>

<p>I&#39;ve been working on the <a href="http://www.cpantesters.org">CPAN Testers</a>
project since 2015. In all that time, I&#39;ve been focused on maintenance
(which has involved more operations/administration tasks than any actual
code changes) and modernization. It&#39;s that modernization effort that has
led to a <a href="http://api.cpantesters.org">new CPAN Testers API</a>.</p>

<p>This new API uses <a href="http://mojolicious.org">the Mojolicious web
framework</a>, along with an
<a href="http://openapis.org">OpenAPI</a> schema to expose all of the most useful
CPAN Testers data. OpenAPI is a specification for web APIs, and there
are tools like <a href="http://swagger.io">Swagger</a> to generate a useful documentation
website from your spec, like the <a href="http://api.cpantesters.org/docs/?url=/v3">CPAN Testers API documentation
website</a>.</p>

                <p><a href="http://preaction.me/blog/2017/07/04/cpan-testers-has-an-api/#section-2">Continue reading...</a></p>
            <p>Tags:
                <a href="http://preaction.me/blog/tag/web/">web</a>
                <a href="http://preaction.me/blog/tag/cpantesters/">cpantesters</a>
            </p>
        ]]></content>
        <updated>2017-07-04T00:00:00Z</updated>
        <category term="web" />
        <category term="cpantesters" />
    </entry>
    <entry>
        <id>http://preaction.me/blog/2017/05/20/2017-perl-toolchain-summit/</id>
        <title>2017 Perl Toolchain Summit</title>
        <link href="http://preaction.me/blog/2017/05/20/2017-perl-toolchain-summit/" rel="alternate" />
        <content type="html"><![CDATA[
            <p>This year I had one goal for <a href="http://cpantesters.org">CPAN Testers</a>:
Replace the current <a href="http://metabase.cpantesters.org">Metabase API</a> with
a new API that did not write to Amazon SimpleDB. The current
high-availability database that raw incoming test reports are written is
Amazon SimpleDB behind an API called Metabase.
<a href="http://metacpan.org/pod/Metabase">Metabase</a> is a highly-flexible data
storage API designed to work with massive, unstructured data sets and
still allow for sane organization and storage of data. Unfortunately,
Amazon SimpleDB is as it says on the tin: Simple. Worse, it&#39;s expensive:
Like most Amazon services, it charges for usage, so there&#39;s a huge
incentive for CPAN Testers to use it as little as possible (which made
some of the code quite obtuse).</p>

<p>So, I made a plan to excise the Metabase. Since we already cached every
raw test report locally in the CPAN Testers MySQL database, I planned to
write a new Metabase API that wrote directly to the cache, and then
adjust the backend processing to read from the cache. I spent the better
part of a month working through all the Metabase APIs, how the data was
stored in the database, and how to translate between a simple JSON
format and the serialized Metabase objects. However, some proper schema
design prevented me from finishing this project: A single <code>NOT NULL</code>
column could not be changed to allow nulls very easily, it being a 600GB
table. The one time where a well-designed schema was a bad thing!</p>

<p>But then <a href="https://github.com/garu">Garu</a>, author of
<a href="https://metacpan.org/pod/cpanm-reporter">cpanm-reporter</a> and
<a href="https://metacpan.org/pod/CPAN::Testers::Common::Client">CPAN::Testers::Common::Client</a>
came up with an idea to make a new test report format. These new reports
would have to be stored in a new place, and I discovered that <a href="https://dev.mysql.com/doc/refman/5.7/en/json.html">MySQL had
recently started building some rich JSON
tooling</a>. Making
a new JSON test report format and storing it in our new
high-availability MySQL cluster seemed like a perfect solution for
storing our raw test reports.</p>

<p>After a few weeks of discussion, I finally realized that it would be an
easier task to make a backwards-compatible Metabase API write to the new
test report MySQL table, even though it increased the amount of work
that needed to be done:</p>

<ul>
<li>Complete the new test report format schema (Garu)</li>
<li>Write the new backwards-compatibility Metabase API (Me)</li>
<li>Write a new test report processor that writes to the old Metabase
cache tables (Joel Berger)</li>
<li>Write a migration script from the old Metabase cache tables to the new
test report JSON object (?)</li>
</ul>

<p>With that plan, I headed for Lyon.</p>

                <p><a href="http://preaction.me/blog/2017/05/20/2017-perl-toolchain-summit/#section-2">Continue reading...</a></p>
            <p>Tags:
                <a href="http://preaction.me/blog/tag/perl/">perl</a>
                <a href="http://preaction.me/blog/tag/cpantesters/">cpantesters</a>
            </p>
        ]]></content>
        <updated>2017-05-20T00:00:00Z</updated>
        <category term="perl" />
        <category term="cpantesters" />
    </entry>
    <entry>
        <id>http://preaction.me/blog/2017/04/23/nerds-rejecting-nerds/</id>
        <title>Nerds Rejecting Nerds</title>
        <link href="http://preaction.me/blog/2017/04/23/nerds-rejecting-nerds/" rel="alternate" />
        <content type="html"><![CDATA[
            <p><a href="https://medium.com/@maradydd/when-nerds-collide-31895b01e68c">https://medium.com/@maradydd/when-nerds-collide-31895b01e68c</a></p>

<p>I was linked to this article after a discussion that was triggered by
a Tweet: <a href="https://twitter.com/shadowcat_mst/status/852265380156510214">https://twitter.com/shadowcat_mst/status/852265380156510214</a></p>

<p>In this article, the author describes a group called &quot;weird nerds&quot;,
later renamed &quot;hackers&quot;, and goes through some of the reasons why this
group is rejecting new members of their community (namely &quot;brogrammers&quot;
and &quot;geek feminists&quot;, a false equivalence if ever there was one).</p>

<p>As someone who fits the author&#39;s idea of a hacker (the classical
definition of hacker, not someone who breaks into computers), and yet
has never felt like part of the hacker community, there are a lot of
things in here that are bad, but I&#39;ll comment for now on a couple
quotes:</p>

                <p><a href="http://preaction.me/blog/2017/04/23/nerds-rejecting-nerds/#section-2">Continue reading...</a></p>
            <p>Tags:
                <a href="http://preaction.me/blog/tag/community/">community</a>
                <a href="http://preaction.me/blog/tag/feminism/">feminism</a>
            </p>
        ]]></content>
        <updated>2017-04-23T00:00:00Z</updated>
        <category term="community" />
        <category term="feminism" />
    </entry>
</feed>

